# Databricks notebook source
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPTIMIZACIÃ“N AVANZADA DELTA LAKE - DEEP DIVE
# VersiÃ³n Community Edition (sin cache operations)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from pyspark.sql.functions import *
import time

print("ğŸ’ OPTIMIZACIÃ“N AVANZADA - DEEP DIVE")
print("="*70)
print("Esto es lo que NO enseÃ±an en tutoriales bÃ¡sicos")
print("="*70 + "\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 1: ENTENDER EL PROBLEMA - "SMALL FILES PROBLEM"
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("ğŸ“š PARTE 1: EL PROBLEMA DE ARCHIVOS PEQUEÃ‘OS\n")
print("-"*70)

# Ver estado ACTUAL de tu tabla Silver
print("ğŸ” Analizando tabla silver_ventas_consolidadas...\n")

detail = spark.sql("DESCRIBE DETAIL silver_ventas_consolidadas").collect()[0]

num_files_before = detail['numFiles']
size_bytes = detail['sizeInBytes']

print(f"ğŸ“Š ESTADO ACTUAL:")
print(f"   NÃºmero de archivos:  {num_files_before}")
print(f"   TamaÃ±o total:        {size_bytes/1024:.2f} KB")
if num_files_before > 0:
    print(f"   TamaÃ±o promedio/archivo: {size_bytes/num_files_before/1024:.2f} KB")

print(f"\nğŸ’¡ CONCEPTO: SMALL FILES PROBLEM")
print(f"-"*70)
print(f"""
PROBLEMA:
  â€¢ Muchos archivos pequeÃ±os = muchas operaciones I/O
  â€¢ Cada archivo requiere overhead de lectura
  â€¢ Spark tiene que abrir cada archivo
  â€¢ â†’ Queries LENTOS ğŸŒ

SOLUCIÃ“N:
  â€¢ OPTIMIZE compacta archivos pequeÃ±os â†’ archivos grandes
  â€¢ Menos archivos = menos overhead
  â€¢ â†’ Queries RÃPIDOS âš¡

REGLA DE ORO:
  â€¢ Archivos ideales: 128MB - 1GB cada uno
  â€¢ Si tienes archivos <10MB â†’ Necesitas OPTIMIZE
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 2: OPTIMIZE - COMPACTACIÃ“N DE ARCHIVOS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸ“š PARTE 2: OPTIMIZE - COMPACTACIÃ“N")
print("="*70 + "\n")

print("ğŸ”§ Â¿QUÃ‰ HACE OPTIMIZE?")
print("-"*70)
print("""
ANTES:
  [file1: 100KB] [file2: 150KB] [file3: 80KB] 
  [file4: 120KB] [file5: 90KB]
  â†’ 5 archivos = 5 operaciones I/O

DESPUÃ‰S DE OPTIMIZE:
  [file_compacted: 540KB]
  â†’ 1 archivo = 1 operaciÃ³n I/O

BENEFICIO: Queries 2-5x MÃS RÃPIDOS âš¡
""")

print("ğŸš€ EJECUTANDO OPTIMIZE...\n")

start = time.time()
spark.sql("OPTIMIZE silver_ventas_consolidadas")
optimize_time = time.time() - start

print(f"âœ… OPTIMIZE completado en {optimize_time:.2f} segundos\n")

# Ver cambio
detail_after = spark.sql("DESCRIBE DETAIL silver_ventas_consolidadas").collect()[0]
num_files_after = detail_after['numFiles']

print(f"ğŸ“Š RESULTADO:")
print(f"   Archivos ANTES:   {num_files_before}")
print(f"   Archivos DESPUÃ‰S: {num_files_after}")
if num_files_before > 0:
    reduction = ((num_files_before - num_files_after)/num_files_before*100)
    print(f"   ReducciÃ³n:        {reduction:.1f}%")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 3: Z-ORDERING - DATA SKIPPING MÃGICO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸ“š PARTE 3: Z-ORDERING - DATA SKIPPING")
print("="*70 + "\n")

print("ğŸ¯ Â¿QUÃ‰ ES Z-ORDERING?")
print("-"*70)
print("""
CONCEPTO: Organizar datos para SALTAR archivos innecesarios

EJEMPLO:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

SIN Z-ORDER (datos mezclados):
  File1: [P123, P456, P789, P123, P456]
  File2: [P789, P123, P456, P789, P123]
  File3: [P456, P789, P123, P456, P789]

Query: WHERE producto_id = 'P123'
â†’ Lee 3 archivos completos (no sabe dÃ³nde estÃ¡ P123)

CON Z-ORDER BY producto_id:
  File1: [P123, P123, P123, P123, P123]  â† Solo P123
  File2: [P456, P456, P456, P456, P456]  â† Solo P456
  File3: [P789, P789, P789, P789, P789]  â† Solo P789

Query: WHERE producto_id = 'P123'
â†’ Lee SOLO File1 (sabe que otros no tienen P123)
â†’ 3x MÃS RÃPIDO âš¡

ESTO SE LLAMA: DATA SKIPPING
Delta Lake mira metadatos y salta archivos innecesarios
""")

print("\nğŸ§ª EXPERIMENTO: Medir diferencia\n")
print("-"*70)

# Query ANTES de Z-Order
print("ğŸ” Test 1: Query SIN Z-Order")
print("   (Midiendo tiempo de ejecuciÃ³n...)\n")

start = time.time()
result_before = spark.table("silver_ventas_consolidadas") \
    .filter(col("producto_id") == "P456") \
    .count()
time_before = time.time() - start

print(f"   Registros: {result_before}")
print(f"   â±ï¸  Tiempo: {time_before:.4f} segundos\n")

# Aplicar Z-Order
print("âš¡ Aplicando Z-ORDER BY producto_id...")
print("   (Esto reorganiza los datos internamente)\n")

start = time.time()
spark.sql("OPTIMIZE silver_ventas_consolidadas ZORDER BY (producto_id)")
zorder_time = time.time() - start

print(f"âœ… Z-ORDER completado en {zorder_time:.2f} segundos\n")

# Query DESPUÃ‰S de Z-Order
print("ğŸ” Test 2: Query CON Z-Order")
print("   (Misma query, datos reorganizados...)\n")

start = time.time()
result_after = spark.table("silver_ventas_consolidadas") \
    .filter(col("producto_id") == "P456") \
    .count()
time_after = time.time() - start

print(f"   Registros: {result_after}")
print(f"   â±ï¸  Tiempo: {time_after:.4f} segundos\n")

# ComparaciÃ³n
if time_before > 0:
    improvement = ((time_before - time_after) / time_before) * 100
    print(f"ğŸ“Š COMPARACIÃ“N:")
    print(f"   Sin Z-Order: {time_before:.4f}s")
    print(f"   Con Z-Order: {time_after:.4f}s")
    if improvement > 0:
        print(f"   Mejora:      {improvement:.1f}% mÃ¡s rÃ¡pido âš¡")
    else:
        print(f"   Diferencia:  {improvement:.1f}%")

print("\nğŸ’¡ NOTA IMPORTANTE:")
print("   Con este dataset pequeÃ±o (15 registros) la mejora es mÃ­nima")
print("   En producciÃ³n con millones de registros:")
print("   â†’ Z-Order puede dar mejoras de 5-10x o mÃ¡s")
print("   â†’ Data skipping es MUY efectivo a escala")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 4: CARDINALIDAD - CUÃNDO USAR Z-ORDER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸ“š PARTE 4: CARDINALIDAD - DECISIONES ESTRATÃ‰GICAS")
print("="*70 + "\n")

print("ğŸ¯ REGLA: Z-Order funciona mejor en ALTA CARDINALIDAD")
print("-"*70)
print("""
CARDINALIDAD = Cantidad de valores Ãºnicos

ALTA CARDINALIDAD (muchos valores Ãºnicos):
  â€¢ user_id: 1 millÃ³n de usuarios diferentes
  â€¢ transaction_id: cada transacciÃ³n es Ãºnica
  â€¢ product_id: 50,000 productos
  â†’ Z-Order BENEFICIA MUCHO âœ…

BAJA CARDINALIDAD (pocos valores Ãºnicos):
  â€¢ status: solo 3 valores (active, pending, closed)
  â€¢ country: solo 10 paÃ­ses
  â€¢ type: solo 5 tipos
  â†’ Z-Order NO ayuda mucho âŒ
  â†’ Mejor usar PARTITION BY
""")

print("\nğŸ“Š ANÃLISIS DE CARDINALIDAD - Tu Dataset:\n")

df = spark.table("silver_ventas_consolidadas")
total_count = df.count()

columnas = ["producto_id", "tienda_id", "fecha"]
print(f"{'Columna':<20} {'Valores Ãºnicos':<15} {'Cardinalidad':<15} {'RecomendaciÃ³n'}")
print("-"*70)

for col_name in columnas:
    distinct_count = df.select(col_name).distinct().count()
    if total_count > 0:
        cardinalidad = distinct_count / total_count
        
        if cardinalidad > 0.5:
            recomendacion = "âœ… Z-Order excelente"
        elif cardinalidad > 0.1:
            recomendacion = "âš ï¸  Z-Order puede ayudar"
        else:
            recomendacion = "âŒ Partition mejor"
        
        print(f"{col_name:<20} {distinct_count:<15} {cardinalidad:<15.1%} {recomendacion}")

print("\nğŸ’¡ ESTRATEGIA:")
print("-"*70)
print("""
âœ… USA Z-ORDER para:
   â€¢ Columnas con ALTA cardinalidad
   â€¢ Que usas frecuentemente en WHERE/JOIN
   â€¢ Ejemplos: user_id, product_id, transaction_id

âœ… USA PARTITION BY para:
   â€¢ Columnas con BAJA cardinalidad
   â€¢ Queries siempre filtradas por ella
   â€¢ Ejemplos: date, country, status

âš ï¸  NO COMBINES:
   â€¢ Si ya usas PARTITION BY fecha
   â€¢ NO hagas Z-ORDER BY fecha tambiÃ©n
   â€¢ SerÃ­a redundante
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 5: ANALYZE TABLE - QUERY OPTIMIZER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸ“š PARTE 5: ANALYZE TABLE - ESTADÃSTICAS")
print("="*70 + "\n")

print("ğŸ“ˆ Â¿POR QUÃ‰ SON CRÃTICAS LAS ESTADÃSTICAS?")
print("-"*70)
print("""
SPARK QUERY OPTIMIZER necesita stats para decidir:

1. TIPO DE JOIN:
   Sin stats: No sabe tamaÃ±os â†’ puede elegir mal
   Con stats: Sabe tabla pequeÃ±a â†’ usa broadcast join (rÃ¡pido)

2. ORDEN DE JOINS:
   Sin stats: Orden random
   Con stats: Joinea tablas pequeÃ±as primero (eficiente)

3. PLAN DE EJECUCIÃ“N:
   Sin stats: Plan genÃ©rico
   Con stats: Plan optimizado para tus datos

RESULTADO: Queries 2-3x mÃ¡s rÃ¡pidos solo con stats actualizadas
""")

print("\nğŸ”§ ACTUALIZANDO ESTADÃSTICAS...\n")

# Compute statistics - nivel bÃ¡sico
print("ğŸ“Š Nivel 1: EstadÃ­sticas tabla completa...")
start = time.time()
spark.sql("ANALYZE TABLE silver_ventas_consolidadas COMPUTE STATISTICS")
time1 = time.time() - start
print(f"   âœ… Completado en {time1:.2f}s\n")

# Compute statistics - por columna
print("ğŸ“Š Nivel 2: EstadÃ­sticas por columna...")
start = time.time()
spark.sql("ANALYZE TABLE silver_ventas_consolidadas COMPUTE STATISTICS FOR ALL COLUMNS")
time2 = time.time() - start
print(f"   âœ… Completado en {time2:.2f}s\n")

print("ğŸ’¡ CUÃNDO EJECUTAR ANALYZE:")
print("-"*70)
print("""
âœ… DespuÃ©s de cargas grandes de datos
âœ… PeriÃ³dicamente (semanal o mensual)
âœ… Cuando queries se vuelven lentos sin razÃ³n aparente
âœ… DespuÃ©s de OPTIMIZE (tamaÃ±os cambiaron)

Costo: Bajo (scan rÃ¡pido)
Beneficio: Query optimizer toma mejores decisiones
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 6: VACUUM - TIME TRAVEL Y STORAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸ“š PARTE 6: VACUUM - BALANCE TIME TRAVEL vs COSTO")
print("="*70 + "\n")

print("ğŸ§¹ Â¿QUÃ‰ ES VACUUM?")
print("-"*70)
print("""
DELTA LAKE mantiene versiones antiguas para TIME TRAVEL:

Cada UPDATE/DELETE/MERGE/OPTIMIZE:
  â†’ Crea nuevos archivos
  â†’ Marca archivos viejos como "invÃ¡lidos"
  â†’ PERO los archivos viejos siguen en storage

TIME TRAVEL permite volver atrÃ¡s:
  SELECT * FROM tabla VERSION AS OF 5
  SELECT * FROM tabla TIMESTAMP AS OF '2024-12-01'

PROBLEMA:
  Con el tiempo acumulas MUCHOS archivos viejos
  â†’ Storage costs suben
  â†’ Necesitas VACUUM para limpiar

TRADE-OFF CRÃTICO:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

RETENCIÃ“N CORTA (7 dÃ­as):
  âœ… Menos storage cost
  âœ… Cleanup frecuente
  âŒ Solo 7 dÃ­as de Time Travel
  
RETENCIÃ“N LARGA (30-90 dÃ­as):
  âŒ MÃ¡s storage cost
  âœ… MÃ¡s Time Travel (auditorÃ­a, compliance)
  âœ… RecuperaciÃ³n de errores

PRODUCCIÃ“N TÃPICA:
  â€¢ Tablas Bronze: 365 dÃ­as (backup completo)
  â€¢ Tablas Silver: 30 dÃ­as (balance)
  â€¢ Tablas Gold: 7 dÃ­as (se regeneran fÃ¡cil)
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 6: VACUUM - TIME TRAVEL Y STORAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸ“š PARTE 6: VACUUM - BALANCE TIME TRAVEL vs COSTO")
print("="*70 + "\n")

print("ğŸ§¹ Â¿QUÃ‰ ES VACUUM?")
print("-"*70)
print("""
DELTA LAKE mantiene versiones antiguas para TIME TRAVEL:

Cada UPDATE/DELETE/MERGE/OPTIMIZE:
  â†’ Crea nuevos archivos
  â†’ Marca archivos viejos como "invÃ¡lidos"
  â†’ PERO los archivos viejos siguen en storage

TIME TRAVEL permite volver atrÃ¡s:
  SELECT * FROM tabla VERSION AS OF 5
  SELECT * FROM tabla TIMESTAMP AS OF '2024-12-01'

PROBLEMA:
  Con el tiempo acumulas MUCHOS archivos viejos
  â†’ Storage costs suben
  â†’ Necesitas VACUUM para limpiar

TRADE-OFF CRÃTICO:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

RETENCIÃ“N CORTA (7 dÃ­as):
  âœ… Menos storage cost
  âœ… Cleanup frecuente
  âŒ Solo 7 dÃ­as de Time Travel
  
RETENCIÃ“N LARGA (30-90 dÃ­as):
  âŒ MÃ¡s storage cost
  âœ… MÃ¡s Time Travel (auditorÃ­a, compliance)
  âœ… RecuperaciÃ³n de errores

PRODUCCIÃ“N TÃPICA:
  â€¢ Tablas Bronze: 365 dÃ­as (backup completo)
  â€¢ Tablas Silver: 30 dÃ­as (balance)
  â€¢ Tablas Gold: 7 dÃ­as (se regeneran fÃ¡cil)

COMANDO PRODUCCIÃ“N:
  VACUUM tabla RETAIN 168 HOURS  -- 7 dÃ­as
  VACUUM tabla RETAIN 720 HOURS  -- 30 dÃ­as
  VACUUM tabla RETAIN 8760 HOURS -- 365 dÃ­as
""")

print("\nâš ï¸  NOTA DATABRICKS COMMUNITY EDITION:")
print("-"*70)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 7: ESTRATEGIA COMPLETA
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸ“š PARTE 7: ESTRATEGIA DE OPTIMIZACIÃ“N - CHECKLIST SENIOR")
print("="*70 + "\n")

print("ğŸ¯ ORDEN CORRECTO DE OPTIMIZACIONES:")
print("-"*70)
print("""
1ï¸âƒ£  DISEÃ‘O INICIAL (al crear tabla):
   â€¢ PARTITION BY (baja cardinalidad: fecha, regiÃ³n)
   â€¢ Decidir estrategia desde el principio

2ï¸âƒ£  OPTIMIZE (periÃ³dico):
   â€¢ Compactar archivos pequeÃ±os
   â€¢ Frecuencia: diario/semanal segÃºn carga
   â€¢ Costo: Medio, beneficio: Alto

3ï¸âƒ£  Z-ORDER (despuÃ©s de OPTIMIZE):
   â€¢ Columnas alta cardinalidad frecuentes en WHERE
   â€¢ MÃ¡ximo 3-4 columnas (despuÃ©s no ayuda)
   â€¢ Costo: Alto, beneficio: Muy alto

4ï¸âƒ£  ANALYZE TABLE (periÃ³dico):
   â€¢ DespuÃ©s de cargas grandes
   â€¢ DespuÃ©s de OPTIMIZE
   â€¢ Frecuencia: semanal/mensual
   â€¢ Costo: Bajo, beneficio: Medio

5ï¸âƒ£  VACUUM (periÃ³dico):
   â€¢ Balance Time Travel vs Storage
   â€¢ Frecuencia: mensual
   â€¢ Costo: Ninguno, beneficio: Ahorro storage
""")

print("\nğŸ¯ PARA ENTREVISTAS - RESPUESTA PERFECTA:")
print("-"*70)
print("""
Pregunta: "Â¿CÃ³mo optimizarÃ­as una tabla lenta?"

Tu respuesta:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"Primero diagnostico el problema:

1. Reviso el query plan con EXPLAIN
   â€¢ Â¿Hace full scan? â†’ Necesita Z-Order o partition
   â€¢ Â¿Lee muchos archivos pequeÃ±os? â†’ Necesita OPTIMIZE

2. Analizo la tabla:
   â€¢ DESCRIBE DETAIL para ver num archivos
   â€¢ Si >1000 archivos pequeÃ±os â†’ OPTIMIZE urgente

3. Aplico optimizaciones en orden:
   â€¢ OPTIMIZE primero (compactar)
   â€¢ Luego Z-ORDER en columnas filtradas frecuente
   â€¢ ANALYZE TABLE para actualizar stats

4. Mido resultados:
   â€¢ Comparo tiempos de query antes/despuÃ©s
   â€¢ Reviso Spark UI para ver mejoras
   â€¢ TÃ­picamente logro mejoras 3-10x

5. EstablecerÃ© schedule:
   â€¢ OPTIMIZE semanal
   â€¢ ANALYZE mensual
   â€¢ VACUUM segÃºn retenciÃ³n necesaria"

â†’ RESPUESTA NIVEL SENIOR âœ…
""")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PARTE 8: APLICAR A TABLAS GOLD
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*70)
print("ğŸš€ OPTIMIZANDO TODAS LAS TABLAS GOLD")
print("="*70 + "\n")

tablas_gold = [
    ("gold_dashboard_ejecutivo", None),
    ("gold_top_productos", "ranking"),
    ("gold_analisis_tiendas", "tienda_id"),
]

for tabla, zorder_col in tablas_gold:
    print(f"âš¡ Optimizando {tabla}...")
    
    # OPTIMIZE
    if zorder_col:
        spark.sql(f"OPTIMIZE {tabla} ZORDER BY ({zorder_col})")
        print(f"   âœ… OPTIMIZE + Z-ORDER por {zorder_col}")
    else:
        spark.sql(f"OPTIMIZE {tabla}")
        print(f"   âœ… OPTIMIZE")
    
    # ANALYZE
    spark.sql(f"ANALYZE TABLE {tabla} COMPUTE STATISTICS")
    print(f"   âœ… ANALYZE")
    print()

print("="*70)
print("ğŸ‰ OPTIMIZACIÃ“N COMPLETA")
print("="*70)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RESUMEN FINAL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ’ LO QUE DOMINASTE HOY:")
print("-"*70)
print("""
âœ… Small Files Problem (por quÃ© queries son lentos)
âœ… OPTIMIZE (compactaciÃ³n de archivos)
âœ… Z-ORDER (data skipping inteligente)
âœ… Cardinalidad (cuÃ¡ndo usar cada tÃ©cnica)
âœ… ANALYZE TABLE (query optimizer)
âœ… VACUUM (Time Travel vs storage cost)
âœ… Estrategia completa de optimizaciÃ³n
âœ… Orden correcto de aplicar optimizaciones

ESTO ES LO QUE DISTINGUE:
  Junior: "Hago pipelines"
  Senior: "Optimizo pipelines y sÃ© por quÃ©" â­

EN ENTREVISTA:
  "Puedo mejorar performance 5-10x con OPTIMIZE, Z-Order
  y estrategia correcta de optimizaciÃ³n Delta Lake"
  
  + MOSTRAR ESTE CÃ“DIGO âœ…
""")

print("\nğŸ“Š ESTADO FINAL:")
print("-"*70)

tablas = [
    "bronze_ventas",
    "silver_ventas_consolidadas",
    "gold_dashboard_ejecutivo",
    "gold_top_productos",
    "gold_analisis_tiendas"
]

for tabla in tablas:
    try:
        info = spark.sql(f"DESCRIBE DETAIL {tabla}").select(
            "numFiles", "sizeInBytes"
        ).collect()[0]
        
        size_kb = info['sizeInBytes'] / 1024
        print(f"   {tabla:35} â†’ {info['numFiles']:2} archivos, {size_kb:.2f} KB")
    except:
        print(f"   {tabla:35} â†’ Error al leer")

print("\nğŸš€ Pipeline optimizado y listo!")
print("="*70)