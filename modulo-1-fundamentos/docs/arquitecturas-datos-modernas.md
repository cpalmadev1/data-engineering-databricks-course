# SECCIÃ“N 1: Data Warehouse vs Data Lake vs Data Lakehouse

## Conceptos Fundamentales: Tipos de Datos

Para entender las arquitecturas de datos modernas, primero debemos conocer 
los diferentes tipos de datos que existen:

### Datos Estructurados
Son aquellos que tienen un formato rÃ­gido y conocido, con columnas y filas 
bien definidas. Ejemplos claros son:
- Tablas en bases de datos relacionales (PostgreSQL, SQL Server)
- Archivos Excel/CSV con columnas fijas
- Datos con esquema predefinido que no cambia

**Ventaja:** FÃ¡ciles de consultar con SQL, rÃ¡pidos, predecibles.
**Desventaja:** Inflexibles - si necesitas agregar un nuevo campo, 
debes modificar el esquema completo.

### Datos Semi-estructurados
Tienen una estructura, pero esta puede ser variable. El mejor ejemplo son 
los archivos JSON, donde:
- Hay una estructura base (etiquetas/campos)
- Pero no todos los registros tienen los mismos campos
- Pueden tener campos anidados o arrays

**Ejemplo JSON:**
```json
// Usuario 1 - tiene telÃ©fono
{"nombre": "Juan", "edad": 30, "telefono": "123456"}

// Usuario 2 - no tiene telÃ©fono, pero tiene ciudad
{"nombre": "MarÃ­a", "edad": 25, "ciudad": "Santiago"}
```

**Ventaja:** Flexibles, fÃ¡ciles de evolucionar.
**Desventaja:** MÃ¡s complejos de consultar, requieren parseo.

### Datos No Estructurados
Son datos sin formato rÃ­gido. Ejemplos:
- ImÃ¡genes (JPG, PNG)
- Videos
- PDFs
- Archivos de texto libre
- Audio

**Ventaja:** Pueden representar cualquier tipo de informaciÃ³n.
**Desventaja:** Muy difÃ­ciles de consultar o analizar directamente.

---

## Data Warehouse (AlmacÃ©n de Datos)

Un Data Warehouse es como una biblioteca perfectamente organizada donde 
cada dato tiene su lugar especÃ­fico y predefinido.

### CaracterÃ­sticas:
- **Solo acepta datos estructurados** (tablas con esquema fijo)
- **Optimizado para consultas rÃ¡pidas** con SQL
- **Alta calidad de datos** - todo estÃ¡ validado y limpio
- **Seguro y confiable** - ideal para reportes crÃ­ticos de negocio

### Â¿Para quÃ© sirve?
Principalmente para **Business Intelligence (BI)**: dashboards, reportes 
ejecutivos, anÃ¡lisis histÃ³rico de mÃ©tricas de negocio.

### Ventajas:
âœ… Consultas muy rÃ¡pidas (optimizado para lectura)
âœ… Datos limpios y confiables
âœ… FÃ¡cil de usar para analistas de negocio (solo SQL)
âœ… Seguridad y control de acceso robusto

### Desventajas:
âŒ **Inflexible:** Si llegan datos en formato diferente, no puede procesarlos
âŒ **Costoso:** Storage y procesamiento son caros
âŒ **Solo datos estructurados:** Si tienes JSONs, imÃ¡genes, logs, no sirve
âŒ **Lento para cambios de esquema:** Agregar una columna nueva puede 
   tomar semanas de trabajo

### Ejemplo Real:
En una tienda retail, el Data Warehouse almacenarÃ­a:
- Tabla de ventas (fecha, monto, producto, tienda)
- Tabla de productos (SKU, nombre, precio, categorÃ­a)
- Tabla de clientes (ID, nombre, email, regiÃ³n)

Todo estructurado, todo con esquema fijo, todo listo para dashboards 
en Power BI.

---

## Data Lake (Lago de Datos)

Un Data Lake es como un gran almacÃ©n donde puedes tirar TODO sin 
organizar mucho. Es un repositorio que acepta **cualquier tipo de dato** 
en su formato original.

### CaracterÃ­sticas:
- **Acepta TODO:** estructurado, semi-estructurado, no estructurado
- **Datos crudos (raw):** Se guardan tal cual llegan, sin transformar
- **Schema-on-read:** La estructura se define cuando lees, no cuando escribes
- **Barato:** Storage muy econÃ³mico (archivos en cloud)

### Â¿Para quÃ© sirve?
- Almacenar grandes volÃºmenes de datos diversos
- Data Science y Machine Learning (que necesitan datos raw)
- AnÃ¡lisis exploratorio
- Backup de todo tipo de informaciÃ³n

### Â¿QuiÃ©n lo usa?
Principalmente **Data Scientists** e **Ingenieros de Datos** que saben 
cÃ³mo procesar datos crudos y extraer valor.

### Ventajas:
âœ… Almacena CUALQUIER tipo de dato
âœ… Muy econÃ³mico (pennies por GB)
âœ… Escalable a petabytes sin problema
âœ… Flexible - no necesitas definir esquema antes

### Desventajas:
âŒ **Riesgo de "Data Swamp" (Pantano de Datos):** 
   Sin organizaciÃ³n, se convierte en un basurero donde nadie encuentra nada
âŒ **Lento para consultar:** 
   Si quieres analizar algo, tienes que leer TODOS los archivos
âŒ **Sin control de calidad:** 
   Pueden haber duplicados, datos corruptos, inconsistencias
âŒ **DifÃ­cil de usar:** 
   Analistas de negocio no pueden trabajar directamente aquÃ­

### Ejemplo Real:
En una tienda retail, el Data Lake almacenarÃ­a:
- Logs de navegaciÃ³n del sitio web (JSON)
- ImÃ¡genes de productos
- Videos de seguridad de tiendas
- CSVs de ventas sin procesar
- PDFs de facturas
- Datos de sensores IoT

Todo mezclado, sin estructura definida.

---

## Data Lakehouse (Casa del Lago)

El Data Lakehouse es la arquitectura **mÃ¡s moderna** (Ãºltimos 3-4 aÃ±os). 
Combina lo mejor de Data Warehouse y Data Lake.

### La Idea Central:
"Â¿Y si pudiera tener la flexibilidad y bajo costo del Data Lake, 
PERO con la calidad, velocidad y confiabilidad del Data Warehouse?"

### Â¿CÃ³mo lo logra?
Mediante tecnologÃ­as como **Delta Lake**, **Apache Iceberg**, o **Apache Hudi** 
que agregan una "capa de gestiÃ³n" sobre archivos simples.

### CaracterÃ­sticas:
- **Storage econÃ³mico** (como Data Lake - archivos en cloud)
- **ACID Transactions** (como Data Warehouse - UPDATE, DELETE, INSERT confiables)
- **Schema Enforcement** (valida que los datos cumplan reglas)
- **Time Travel** (puedes ver versiones anteriores de los datos)
- **Optimizado para consultas** (Ã­ndices, estadÃ­sticas, particionamiento)

### Ventajas:
âœ… **EconÃ³mico** como Data Lake
âœ… **RÃ¡pido** como Data Warehouse  
âœ… **Flexible** - soporta datos estructurados y semi-estructurados
âœ… **Confiable** - transacciones ACID, no hay corrupciÃ³n de datos
âœ… **Versionado** - puedes volver atrÃ¡s en el tiempo
âœ… **Unificado** - sirve para BI Y para Machine Learning

### Desventajas:
âŒ **MÃ¡s complejo de configurar** (requiere conocimientos tÃ©cnicos)
âŒ **TecnologÃ­a relativamente nueva** (menos madurez que Data Warehouse)
âŒ **Requiere herramientas especÃ­ficas** (Databricks, Spark, etc.)

### Ejemplo Real con Delta Lake:
```python
# Puedes hacer UPDATE directamente en archivos (Â¡imposible en Data Lake tradicional!)
deltaTable.update(
  condition = "fecha < '2024-01-01'", 
  set = {"estado": "'procesado'"}
)

# Puedes ver versiones anteriores (Time Travel)
df = spark.read.format("delta") \
  .option("versionAsOf", 5) \
  .load("/data/ventas")

# Puedes hacer MERGE (UPSERT) atÃ³mico
deltaTable.merge(nuevos_datos, "id = id_nuevo") \
  .whenMatchedUpdate(...) \
  .whenNotMatchedInsert(...) \
  .execute()
```

### Â¿Por quÃ© es el futuro?
Porque resuelve el problema histÃ³rico de tener que elegir entre:
- **Barato pero desordenado** (Data Lake)
- **Ordenado pero caro** (Data Warehouse)

Con Lakehouse tienes **barato Y ordenado**. ğŸ¯

---

## Tabla Comparativa

| CaracterÃ­stica | Data Warehouse | Data Lake | Data Lakehouse |
|----------------|----------------|-----------|----------------|
| **Tipos de datos** | Solo estructurados | Todos | Todos |
| **Formato** | Tablas en BD | Archivos (CSV, JSON, Parquet) | Archivos + capa de gestiÃ³n (Delta) |
| **Esquema** | Schema-on-write (fijo) | Schema-on-read (flexible) | Schema enforcement (validado) |
| **Costo** | Alto | Bajo | Bajo-Medio |
| **Velocidad consultas** | Muy rÃ¡pida | Lenta | RÃ¡pida |
| **ACID** | SÃ­ | No | SÃ­ |
| **Calidad datos** | Alta | Baja (riesgo pantano) | Alta |
| **Uso principal** | BI, Reportes | Data Science, ML | BI + ML + Todo |
| **Usuarios** | Analistas negocio | Data Scientists, DE | Todos |
| **Ejemplos** | Snowflake, BigQuery | AWS S3, Azure Blob | Databricks, Delta Lake |

---

## Diagrama Visual

<img width="1200" height="630" alt="image" src="https://github.com/user-attachments/assets/5f8412d3-faef-4b44-ba83-575b62324eeb" />
Fuente: Databricks

    â†‘ Lo mejor de ambos mundos

# SECCIÃ“N 2: Medallion Architecture (Bronze â†’ Silver â†’ Gold)

## Â¿QuÃ© es Medallion Architecture?

Medallion Architecture describe una serie de **capas de datos** que denotan 
la **calidad y el nivel de procesamiento** de los datos almacenados en un 
Data Lakehouse.

Se usa para **organizar los datos lÃ³gicamente** a medida que son procesados, 
desde su estado mÃ¡s crudo hasta su forma mÃ¡s refinada y lista para consumo 
empresarial.

## Objetivo Principal

**Mejorar de forma incremental y progresiva la calidad de los datos** a medida 
que fluyen a travÃ©s de cada capa de la arquitectura.

Cada capa tiene un propÃ³sito especÃ­fico y usuarios especÃ­ficos.

---

## ğŸ¥‰ Capa BRONZE (Bronce) - Datos Crudos

### Â¿QuÃ© es?
La capa Bronze es la **zona de aterrizaje** donde llegan los datos tal cual 
vienen de las fuentes originales, sin transformaciones.

### CaracterÃ­sticas:
- **Datos raw (crudos):** Sin limpiar, sin validar, sin transformar
- **De TODO tipo:** Estructurados, semi-estructurados, no estructurados
- **HistÃ³rico completo:** Se guardan TODOS los datos que llegan
- **Inmutable:** Una vez guardado, no se modifica (solo se agrega)
- **Con metadata:** Fecha de ingesta, fuente origen, versiÃ³n

### Â¿Para quÃ© sirve?
- **Backup histÃ³rico:** Siempre puedes volver a la fuente original
- **Reprocesamiento:** Si algo falla en capas superiores, reprocesas desde Bronze
- **AuditorÃ­a:** Tienes registro de quÃ© datos llegaron y cuÃ¡ndo
- **Data Science exploratorio:** A veces necesitas los datos crudos

### Ejemplo Real - Retail:
```
ğŸ“¦ bronze/
  â”œâ”€â”€ ventas_pos/
  â”‚   â”œâ”€â”€ 2024-11-29/
  â”‚   â”‚   â”œâ”€â”€ tienda_001.json  â† Datos tal cual salen del POS
  â”‚   â”‚   â”œâ”€â”€ tienda_002.json  â† Pueden tener errores, duplicados
  â”‚   â”‚   â””â”€â”€ tienda_003.json  â† Diferentes formatos incluso
  â”‚   â””â”€â”€ metadata/
  â”‚       â””â”€â”€ ingesta_log.json  â† Registro de quÃ© se ingiriÃ³
  â”œâ”€â”€ clickstream_web/
  â”‚   â””â”€â”€ 2024-11-29/
  â”‚       â””â”€â”€ events.json  â† Logs raw del sitio web
  â””â”€â”€ imagenes_productos/
      â””â”€â”€ nuevos/  â† ImÃ¡genes sin procesar
```

### Â¿QuiÃ©n lo usa?
- Data Engineers (para debugging)
- Data Scientists (para anÃ¡lisis exploratorio)
- Sistemas automatizados (para reprocesar)

### Formato recomendado:
- **Delta Lake** (para ACID y versionado)
- **Particionado por fecha** (ej: year=2024/month=11/day=29)
- **CompresiÃ³n:** Snappy o Gzip

---

## ğŸ¥ˆ Capa SILVER (Plata) - Datos Limpios

### Â¿QuÃ© es?
La capa Silver contiene datos **limpios, validados y enriquecidos**, pero 
aÃºn en un nivel tÃ©cnico (no modelado para negocio).

### CaracterÃ­sticas:
- **Datos limpios:** Sin duplicados, sin nulos crÃ­ticos
- **Validados:** Pasan reglas de calidad
- **Normalizados:** Formatos estandarizados (fechas, monedas, etc.)
- **Enriquecidos:** Pueden tener joins con otras tablas
- **Tipados correctamente:** String, Int, Date, etc. bien definidos

### Transformaciones tÃ­picas:
```python
# Ejemplo de transformaciones Bronze â†’ Silver

# 1. Limpiar datos
df_silver = df_bronze \
  .dropDuplicates(["transaction_id"]) \  # Eliminar duplicados
  .filter(col("monto") > 0) \             # Quitar montos negativos/cero
  .filter(col("fecha").isNotNull()) \     # Quitar registros sin fecha
  
# 2. Normalizar formatos
  .withColumn("fecha", to_date("fecha", "yyyy-MM-dd")) \  # Estandarizar fechas
  .withColumn("monto", round(col("monto"), 2)) \          # 2 decimales
  
# 3. Validar reglas de negocio
  .filter(col("cantidad") <= 1000) \  # Cantidad mÃ¡xima razonable
  .filter(col("monto") <= 10000000) \ # Monto mÃ¡ximo razonable
  
# 4. Enriquecer
  .join(productos_dim, "producto_id") \  # Agregar info de producto
  .join(tiendas_dim, "tienda_id")        # Agregar info de tienda
```

### Ejemplo Real - Retail:
```
ğŸ“¦ silver/
  â”œâ”€â”€ ventas_consolidadas/
  â”‚   â””â”€â”€ ventas_limpias.delta  â† Todas las tiendas, limpio, sin duplicados
  â”œâ”€â”€ clientes_enriquecidos/
  â”‚   â””â”€â”€ clientes.delta  â† Con segmentaciÃ³n, RFM, etc.
  â””â”€â”€ productos_master/
      â””â”€â”€ productos.delta  â† CatÃ¡logo limpio y actualizado
```

### Â¿Para quÃ© sirve?
- **Base para anÃ¡lisis tÃ©cnico:** Data Scientists trabajan aquÃ­
- **Feature engineering para ML:** Se crean features desde Silver
- **IntegraciÃ³n de sistemas:** Otros sistemas pueden consumir Silver
- **Base para capa Gold:** Gold se construye desde Silver

### Â¿QuiÃ©n lo usa?
- Data Engineers (construyen pipelines)
- Data Scientists (entrenan modelos)
- Sistemas automatizados (APIs, integraciones)

---

## ğŸ¥‡ Capa GOLD (Oro) - Datos Modelados para Negocio

### Â¿QuÃ© es?
La capa Gold contiene datos **altamente refinados, agregados y modelados** 
especÃ­ficamente para casos de uso de negocio.

### CaracterÃ­sticas:
- **Modelado dimensional:** Esquemas estrella o copo de nieve
- **Agregaciones pre-calculadas:** KPIs, mÃ©tricas, totales
- **Optimizado para BI:** Estructurado para dashboards y reportes
- **Lenguaje de negocio:** Nombres de columnas que entiende el negocio
- **Menos volumen:** Solo lo necesario para anÃ¡lisis

### Ejemplos de datasets Gold:
```
ğŸ“¦ gold/
  â”œâ”€â”€ kpis_ventas_diarias/
  â”‚   â””â”€â”€ ventas_agregadas.delta
  â”‚       Columnas: fecha, region, categoria, 
  â”‚                 total_ventas, total_unidades, 
  â”‚                 ticket_promedio, num_transacciones
  â”‚
  â”œâ”€â”€ dashboard_ejecutivo/
  â”‚   â””â”€â”€ metricas_mensuales.delta
  â”‚       Columnas: mes, ventas_totales, margen_bruto,
  â”‚                 crecimiento_vs_aÃ±o_anterior
  â”‚
  â”œâ”€â”€ segmentacion_clientes/
  â”‚   â””â”€â”€ clientes_rfm.delta
  â”‚       Columnas: cliente_id, segmento_rfm, 
  â”‚                 valor_lifetime, probabilidad_churn
  â”‚
  â””â”€â”€ reporte_inventario/
      â””â”€â”€ stock_por_tienda.delta
          Columnas: tienda, producto, stock_actual,
                    dias_stock, punto_reorden
```

### Ejemplo de transformaciÃ³n Silver â†’ Gold:
```python
# Crear tabla agregada para dashboard de ventas

df_gold_ventas_diarias = df_silver_ventas \
  .groupBy("fecha", "region", "categoria") \
  .agg(
    sum("monto").alias("total_ventas"),
    sum("cantidad").alias("total_unidades"),
    avg("monto").alias("ticket_promedio"),
    countDistinct("transaction_id").alias("num_transacciones"),
    countDistinct("cliente_id").alias("clientes_unicos")
  ) \
  .withColumn("aÃ±o", year("fecha")) \
  .withColumn("mes", month("fecha")) \
  .withColumn("trimestre", quarter("fecha"))

# Guardar en Gold
df_gold_ventas_diarias.write \
  .format("delta") \
  .mode("overwrite") \
  .partitionBy("aÃ±o", "mes") \
  .save("/gold/kpis_ventas_diarias")
```

### Â¿Para quÃ© sirve?
- **Dashboards en Power BI / Tableau**
- **Reportes ejecutivos**
- **AnÃ¡lisis de negocio ad-hoc**
- **KPIs para monitoreo**

### Â¿QuiÃ©n lo usa?
- **Analistas de negocio**
- **Ejecutivos (C-level)**
- **Product Managers**
- **Equipos de ventas/marketing**

**CaracterÃ­stica clave:** Estos usuarios **NO saben SQL avanzado ni Python**. 
Necesitan datos ya procesados y listos para consumir.

---

## ğŸ“Š ComparaciÃ³n de las 3 Capas

| Aspecto | Bronze ğŸ¥‰ | Silver ğŸ¥ˆ | Gold ğŸ¥‡ |
|---------|----------|----------|---------|
| **Estado datos** | Crudos, raw | Limpios, validados | Agregados, modelados |
| **Volumen** | Muy alto (TB-PB) | Alto (GB-TB) | Bajo-Medio (MB-GB) |
| **Calidad** | Baja (todo entra) | Media-Alta | Muy alta |
| **TransformaciÃ³n** | Ninguna | Limpieza + validaciÃ³n | AgregaciÃ³n + modelado |
| **Usuarios** | DE, DS avanzados | DE, DS | Analistas, negocio |
| **ActualizaciÃ³n** | Cada ingesta | Diaria/horaria | Diaria/semanal |
| **PropÃ³sito** | Backup + auditorÃ­a | Base para anÃ¡lisis | BI + dashboards |
| **Esquema** | Flexible | Estructurado | Dimensional |
| **Ejemplo tabla** | ventas_raw | ventas_limpias | kpi_ventas_diarias |

---

## ğŸ”„ Flujo Completo: Bronze â†’ Silver â†’ Gold

### Ejemplo: Procesamiento de Ventas Retail
```
FUENTE: Sistemas POS de 500 tiendas
    â†“
    â†“ (Ingesta cada 15 minutos)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BRONZE - Datos Raw                 â”‚
â”‚  - 500 archivos JSON por dÃ­a        â”‚
â”‚  - Con errores, duplicados          â”‚
â”‚  - Formatos inconsistentes          â”‚
â”‚  - ~10 GB/dÃ­a                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    â†“ (Pipeline de limpieza - cada hora)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SILVER - Datos Limpios             â”‚
â”‚  - 1 tabla consolidada              â”‚
â”‚  - Sin duplicados                   â”‚
â”‚  - Formatos estandarizados          â”‚
â”‚  - Enriquecido con dimensiones      â”‚
â”‚  - ~5 GB/dÃ­a (comprimido)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    â†“ (Agregaciones - cada 6 horas)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GOLD - KPIs y MÃ©tricas             â”‚
â”‚  - Ventas por regiÃ³n/dÃ­a            â”‚
â”‚  - Top productos                    â”‚
â”‚  - SegmentaciÃ³n clientes            â”‚
â”‚  - Dashboard ejecutivo              â”‚
â”‚  - ~100 MB/dÃ­a                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
    â†“
    â†“
Power BI Dashboard â† Analistas de negocio
```

---

## ğŸ’¡ Mejores PrÃ¡cticas

### 1. **Inmutabilidad en Bronze**
âŒ Nunca modifiques datos en Bronze
âœ… Siempre agrega nuevos datos con timestamp

### 2. **Idempotencia en pipelines**
Los pipelines Silver y Gold deben ser **idempotentes**: 
si los ejecutas 2 veces con los mismos datos de entrada, 
el resultado debe ser el mismo.

### 3. **Versionado con Delta Lake**
Usa Delta Lake en todas las capas para:
- Time Travel (volver a versiones anteriores)
- ACID transactions (no corrupciÃ³n de datos)
- Schema evolution (agregar columnas sin romper nada)

### 4. **Particionamiento inteligente**
```
Bronze:  Particionar por fecha de ingesta
         /bronze/ventas/year=2024/month=11/day=29/

Silver:  Particionar por fecha lÃ³gica de negocio
         /silver/ventas/fecha=2024-11-29/

Gold:    Particionar por dimensiones de consulta frecuente
         /gold/ventas/region=santiago/year=2024/month=11/
```

### 5. **Data Quality Checks**
Implementar validaciones automÃ¡ticas:
```python
# Ejemplo con Great Expectations
expectation_suite = df.expect_column_values_to_not_be_null("transaction_id")
expectation_suite = df.expect_column_values_to_be_between("monto", 0, 10000000)
expectation_suite = df.expect_column_values_to_be_in_set("estado", ["completado", "cancelado"])
```

## ğŸ“ Diagrama Medallion Architecture

<img width="1200" height="619" alt="image" src="https://github.com/user-attachments/assets/0db8bfeb-4aaf-4dee-8873-a366b525a700" />


**Fuente:** Databricks - Data Lakehouse Architecture

El diagrama muestra claramente cÃ³mo:
- Bronze acepta TODO tipo de datos
- Silver limpia y valida
- Gold agrega y modela para consumo empresarial
- 

# SECCIÃ“N 3: Batch vs Streaming

## La DecisiÃ³n Fundamental

Al diseÃ±ar un sistema de procesamiento de datos, una de las primeras decisiones 
que debemos tomar es: **Â¿Batch o Streaming?**

La respuesta depende de **quÃ© tan rÃ¡pido necesitas la informaciÃ³n**.

---

## Batch Processing (Procesamiento por Lotes)

### Â¿QuÃ© es?

Batch processing es el procesamiento de datos en **bloques grandes** en 
**intervalos programados**. Los datos se acumulan durante un perÃ­odo de tiempo 
y luego se procesan todos juntos.

### CaracterÃ­sticas Clave:

- **EjecuciÃ³n programada:** Se ejecuta en horarios especÃ­ficos (como un cron job)
- **Procesa datos histÃ³ricos:** Trabaja con datos del pasado (ayer, semana pasada, mes anterior)
- **Alto volumen por ejecuciÃ³n:** Procesa muchos datos de una sola vez
- **Latencia aceptable:** Los resultados pueden esperar horas o dÃ­as

### Ejemplo Real - Retail:
```
Escenario: Reporte de ventas diarias

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Durante el dÃ­a (00:00 - 23:59)     â”‚
â”‚  - Las ventas se van acumulando     â”‚
â”‚  - Se guardan en Bronze (raw)       â”‚
â”‚  - NO se procesan todavÃ­a           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        A las 01:00 AM
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Job Batch programado se ejecuta    â”‚
â”‚  1. Lee TODAS las ventas del dÃ­a    â”‚
â”‚  2. Limpia y valida (Bronzeâ†’Silver) â”‚
â”‚  3. Agrega por tienda (Silverâ†’Gold) â”‚
â”‚  4. Actualiza dashboard             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
        Dashboard listo a las 01:30 AM
```

**Ventajas:**
- âœ… **Simple de implementar:** Un script que corre una vez al dÃ­a
- âœ… **EconÃ³mico:** Solo usa recursos cuando se ejecuta (no 24/7)
- âœ… **Eficiente para grandes volÃºmenes:** Procesa millones de registros de una vez
- âœ… **FÃ¡cil de debuggear:** Si algo falla, puedes ver todo el lote
- âœ… **Reproducible:** Puedes re-ejecutar el mismo lote si hay errores

**Desventajas:**
- âŒ **Latencia alta:** Los datos pueden tener horas de retraso
- âŒ **No apto para tiempo real:** Si necesitas alertas inmediatas, no sirve
- âŒ **Todo o nada:** Si falla, tienes que reprocesar todo el lote

### Casos de Uso TÃ­picos:

- ğŸ“Š **Reportes diarios/semanales/mensuales**
  - Ventas totales del dÃ­a anterior
  - KPIs mensuales para ejecutivos
  
- ğŸ“ˆ **ETL tradicional**
  - Mover datos de BD transaccional a Data Warehouse
  - Consolidar datos de mÃºltiples fuentes
  
- ğŸ§® **CÃ¡lculos pesados**
  - Entrenar modelos de Machine Learning
  - AnÃ¡lisis histÃ³rico profundo
  
- ğŸ’¾ **Backups y archivos**
  - Exportar datos para auditorÃ­a
  - Generar snapshots diarios

### Herramientas Comunes:

- **Azure Databricks Jobs**
- **Azure Data Factory** (orquestaciÃ³n de pipelines)
- **Apache Spark Batch**
- **Cron jobs + Scripts Python**

---

## Streaming Processing (Procesamiento Continuo)

### Â¿QuÃ© es?

Streaming processing es el procesamiento **continuo y en tiempo real** de datos 
a medida que llegan. Cada evento se procesa inmediatamente (o en ventanas de 
pocos segundos).

### CaracterÃ­sticas Clave:

- **EjecuciÃ³n continua 24/7:** Siempre estÃ¡ corriendo, esperando datos
- **Procesa eventos en vivo:** Datos que estÃ¡n ocurriendo AHORA
- **Bajo volumen por evento:** Procesa registros uno a uno (o micro-batches)
- **Latencia muy baja:** Segundos o milisegundos

### Ejemplo Real - DetecciÃ³n de Fraude:
```
Usuario hace compra con tarjeta de crÃ©dito
              â†“ (< 1 segundo)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evento llega al sistema streaming  â”‚
â”‚  - Monto: $5,000                    â”‚
â”‚  - UbicaciÃ³n: PaÃ­s extranjero       â”‚
â”‚  - Horario: 3 AM                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (inmediato)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Sistema analiza en tiempo real     â”‚
â”‚  - Compara con patrÃ³n usual         â”‚
â”‚  - Detecta: ubicaciÃ³n inusual       â”‚
â”‚  - Detecta: monto alto              â”‚
â”‚  - DECISIÃ“N: Posible fraude         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“ (2-3 segundos)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ACCIÃ“N INMEDIATA                   â”‚
â”‚  - Bloquea la transacciÃ³n           â”‚
â”‚  - EnvÃ­a SMS al usuario             â”‚
â”‚  - Alerta al equipo de seguridad    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Todo esto pasa en < 5 segundos** mientras el usuario aÃºn estÃ¡ en la tienda.

**Ventajas:**
- âœ… **Respuesta inmediata:** AcciÃ³n en segundos
- âœ… **Experiencia de usuario mejorada:** Dashboards que se actualizan solos
- âœ… **Detecta problemas rÃ¡pido:** Alertas en tiempo real
- âœ… **Datos siempre frescos:** No hay "datos de ayer"

**Desventajas:**
- âŒ **MÃ¡s complejo:** Requiere arquitectura especializada
- âŒ **MÃ¡s costoso:** Recursos corriendo 24/7
- âŒ **MÃ¡s difÃ­cil de debuggear:** Los eventos ya pasaron, no puedes "pausar"
- âŒ **Requiere mÃ¡s monitoreo:** Debe estar siempre funcionando

### Casos de Uso TÃ­picos:

- ğŸš¨ **DetecciÃ³n de fraude en tiempo real**
  - Tarjetas de crÃ©dito
  - Transacciones bancarias
  
- ğŸ“± **Recomendaciones instantÃ¡neas**
  - "Usuarios que vieron esto tambiÃ©n vieron..."
  - PersonalizaciÃ³n de contenido en tiempo real
  
- ğŸ“Š **Dashboards en vivo**
  - Ventas actuales (Ãºltimos 5 minutos)
  - TrÃ¡fico del sitio web en tiempo real
  
- ğŸ”” **Alertas y notificaciones**
  - Sistema caÃ­do â†’ alerta inmediata
  - Inventario bajo â†’ notificar a compras
  
- ğŸŒ¡ï¸ **IoT y sensores**
  - Monitoreo de temperatura en tiempo real
  - DetecciÃ³n de fallas en maquinaria

### Herramientas Comunes:

- **Databricks Structured Streaming**
- **Apache Kafka** (event streaming)
- **Azure Event Hubs**
- **Apache Flink**
- **Spark Streaming**

---

## Tabla Comparativa Completa

| Aspecto | Batch | Streaming |
|---------|-------|-----------|
| **Â¿CuÃ¡ndo se ejecuta?** | Horarios programados (cron) | Continuamente (24/7) |
| **Latencia** | Horas a dÃ­as | Segundos a milisegundos |
| **Volumen por ejecuciÃ³n** | Alto (millones de registros) | Bajo (eventos individuales) |
| **Datos procesados** | HistÃ³ricos (ayer, semana pasada) | En vivo (ahora mismo) |
| **Complejidad** | Baja - Media | Alta |
| **Costo** | Bajo (solo cuando corre) | Alto (siempre corriendo) |
| **Debugging** | FÃ¡cil (puedes re-ejecutar) | DifÃ­cil (eventos ya pasaron) |
| **Uso de recursos** | Picos altos pero temporales | Uso constante pero moderado |
| **Ejemplos** | Reportes EOD, ETL nocturno | Fraude, alertas, dashboards RT |

---

## Â¿CÃ³mo Decidir? - Ãrbol de DecisiÃ³n
```
Â¿Necesitas la informaciÃ³n AHORA (< 1 minuto)?
â”‚
â”œâ”€ SÃ â†’ Â¿Es crÃ­tico para el negocio actuar inmediatamente?
â”‚       â”‚
â”‚       â”œâ”€ SÃ â†’ STREAMING
â”‚       â”‚       Ejemplos: 
â”‚       â”‚       - DetecciÃ³n de fraude
â”‚       â”‚       - Alertas de sistema caÃ­do
â”‚       â”‚       - Recomendaciones en vivo
â”‚       â”‚
â”‚       â””â”€ NO â†’ BATCH (con ejecuciones frecuentes)
â”‚               Ejemplo: Dashboard que se actualiza cada 15 min
â”‚
â””â”€ NO â†’ Â¿Los datos pueden esperar horas/dÃ­as?
        â”‚
        â”œâ”€ SÃ â†’ BATCH
        â”‚       Ejemplos:
        â”‚       - Reportes diarios
        â”‚       - ETL nocturno
        â”‚       - AnÃ¡lisis histÃ³rico
        â”‚
        â””â”€ NO â†’ MICRO-BATCH o STREAMING con ventanas
                Ejemplo: Dashboard que se actualiza cada 5 minutos
```

---

## Arquitecturas HÃ­bridas (Lambda Architecture)

En la prÃ¡ctica, **muchas empresas usan AMBOS** en paralelo:
```
                    Fuente de Datos (ej: Ventas)
                            â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â†“               â†“
            STREAMING PATH      BATCH PATH
                    â†“               â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Kafka/Event Hub â”‚  â”‚ Bronze Layer    â”‚
          â”‚ Procesa eventos â”‚  â”‚ Acumula datos   â”‚
          â”‚ en tiempo real  â”‚  â”‚ del dÃ­a         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“                     â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Tabla STREAM    â”‚  â”‚ Job programado  â”‚
          â”‚ Datos Ãºltimos   â”‚  â”‚ 01:00 AM        â”‚
          â”‚ 5 minutos       â”‚  â”‚ Procesa todo    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“                     â†“
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ Dashboard VIVO  â”‚  â”‚ Tabla histÃ³rica â”‚
          â”‚ "Ventas ahora"  â”‚  â”‚ "Ventas ayer"   â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ejemplo Real - Dashboard de Ventas:

**Pantalla 1 (Streaming):**
- "Ventas Ãºltimos 5 minutos: $12,450"
- "Transacciones en curso: 47"
- ActualizaciÃ³n: Cada 10 segundos

**Pantalla 2 (Batch):**
- "Ventas totales ayer: $1,245,890"
- "ComparaciÃ³n vs mes pasado: +15%"
- ActualizaciÃ³n: Una vez al dÃ­a (01:00 AM)

---

## En este Curso: Â¿QuÃ© Usaremos?

### MÃ³dulo 3-4: Principalmente BATCH
- Aprenderemos a crear pipelines Bronze â†’ Silver â†’ Gold
- Jobs programados con Databricks
- Procesamiento de grandes volÃºmenes

### MÃ³dulo 5: IntroducciÃ³n a STREAMING
- Structured Streaming en Databricks
- Procesamiento de eventos en tiempo real
- Casos de uso hÃ­bridos

**RazÃ³n:** El 80% de los trabajos de Data Engineering en el mundo real 
son **Batch**. Streaming es mÃ¡s especializado y se usa cuando realmente 
se necesita.

---

## ConclusiÃ³n

La decisiÃ³n entre Batch y Streaming no es "uno u otro", sino **"cuÃ¡ndo usar 
cada uno"**.

**Regla de oro:**
- âœ… **Batch por defecto** â†’ Simple, econÃ³mico, suficiente para mayorÃ­a de casos
- âœ… **Streaming solo cuando sea necesario** â†’ Cuando tiempo real es crÃ­tico

La mayorÃ­a de sistemas modernos usan **ambos en paralelo**, procesando lo 
crÃ­tico en streaming y lo demÃ¡s en batch.
```
