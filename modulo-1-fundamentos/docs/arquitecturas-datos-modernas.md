# SECCI√ìN 1: Data Warehouse vs Data Lake vs Data Lakehouse

## Conceptos Fundamentales: Tipos de Datos

Para entender las arquitecturas de datos modernas, primero debemos conocer 
los diferentes tipos de datos que existen:

### Datos Estructurados
Son aquellos que tienen un formato r√≠gido y conocido, con columnas y filas 
bien definidas. Ejemplos claros son:
- Tablas en bases de datos relacionales (PostgreSQL, SQL Server)
- Archivos Excel/CSV con columnas fijas
- Datos con esquema predefinido que no cambia

**Ventaja:** F√°ciles de consultar con SQL, r√°pidos, predecibles.
**Desventaja:** Inflexibles - si necesitas agregar un nuevo campo, 
debes modificar el esquema completo.

### Datos Semi-estructurados
Tienen una estructura, pero esta puede ser variable. El mejor ejemplo son 
los archivos JSON, donde:
- Hay una estructura base (etiquetas/campos)
- Pero no todos los registros tienen los mismos campos
- Pueden tener campos anidados o arrays

**Ejemplo JSON:**
```json
// Usuario 1 - tiene tel√©fono
{"nombre": "Juan", "edad": 30, "telefono": "123456"}

// Usuario 2 - no tiene tel√©fono, pero tiene ciudad
{"nombre": "Mar√≠a", "edad": 25, "ciudad": "Santiago"}
```

**Ventaja:** Flexibles, f√°ciles de evolucionar.
**Desventaja:** M√°s complejos de consultar, requieren parseo.

### Datos No Estructurados
Son datos sin formato r√≠gido. Ejemplos:
- Im√°genes (JPG, PNG)
- Videos
- PDFs
- Archivos de texto libre
- Audio

**Ventaja:** Pueden representar cualquier tipo de informaci√≥n.
**Desventaja:** Muy dif√≠ciles de consultar o analizar directamente.

---

## Data Warehouse (Almac√©n de Datos)

Un Data Warehouse es como una biblioteca perfectamente organizada donde 
cada dato tiene su lugar espec√≠fico y predefinido.

### Caracter√≠sticas:
- **Solo acepta datos estructurados** (tablas con esquema fijo)
- **Optimizado para consultas r√°pidas** con SQL
- **Alta calidad de datos** - todo est√° validado y limpio
- **Seguro y confiable** - ideal para reportes cr√≠ticos de negocio

### ¬øPara qu√© sirve?
Principalmente para **Business Intelligence (BI)**: dashboards, reportes 
ejecutivos, an√°lisis hist√≥rico de m√©tricas de negocio.

### Ventajas:
‚úÖ Consultas muy r√°pidas (optimizado para lectura)
‚úÖ Datos limpios y confiables
‚úÖ F√°cil de usar para analistas de negocio (solo SQL)
‚úÖ Seguridad y control de acceso robusto

### Desventajas:
‚ùå **Inflexible:** Si llegan datos en formato diferente, no puede procesarlos
‚ùå **Costoso:** Storage y procesamiento son caros
‚ùå **Solo datos estructurados:** Si tienes JSONs, im√°genes, logs, no sirve
‚ùå **Lento para cambios de esquema:** Agregar una columna nueva puede 
   tomar semanas de trabajo

### Ejemplo Real:
En una tienda retail, el Data Warehouse almacenar√≠a:
- Tabla de ventas (fecha, monto, producto, tienda)
- Tabla de productos (SKU, nombre, precio, categor√≠a)
- Tabla de clientes (ID, nombre, email, regi√≥n)

Todo estructurado, todo con esquema fijo, todo listo para dashboards 
en Power BI.

---

## Data Lake (Lago de Datos)

Un Data Lake es como un gran almac√©n donde puedes tirar TODO sin 
organizar mucho. Es un repositorio que acepta **cualquier tipo de dato** 
en su formato original.

### Caracter√≠sticas:
- **Acepta TODO:** estructurado, semi-estructurado, no estructurado
- **Datos crudos (raw):** Se guardan tal cual llegan, sin transformar
- **Schema-on-read:** La estructura se define cuando lees, no cuando escribes
- **Barato:** Storage muy econ√≥mico (archivos en cloud)

### ¬øPara qu√© sirve?
- Almacenar grandes vol√∫menes de datos diversos
- Data Science y Machine Learning (que necesitan datos raw)
- An√°lisis exploratorio
- Backup de todo tipo de informaci√≥n

### ¬øQui√©n lo usa?
Principalmente **Data Scientists** e **Ingenieros de Datos** que saben 
c√≥mo procesar datos crudos y extraer valor.

### Ventajas:
‚úÖ Almacena CUALQUIER tipo de dato
‚úÖ Muy econ√≥mico (pennies por GB)
‚úÖ Escalable a petabytes sin problema
‚úÖ Flexible - no necesitas definir esquema antes

### Desventajas:
‚ùå **Riesgo de "Data Swamp" (Pantano de Datos):** 
   Sin organizaci√≥n, se convierte en un basurero donde nadie encuentra nada
‚ùå **Lento para consultar:** 
   Si quieres analizar algo, tienes que leer TODOS los archivos
‚ùå **Sin control de calidad:** 
   Pueden haber duplicados, datos corruptos, inconsistencias
‚ùå **Dif√≠cil de usar:** 
   Analistas de negocio no pueden trabajar directamente aqu√≠

### Ejemplo Real:
En una tienda retail, el Data Lake almacenar√≠a:
- Logs de navegaci√≥n del sitio web (JSON)
- Im√°genes de productos
- Videos de seguridad de tiendas
- CSVs de ventas sin procesar
- PDFs de facturas
- Datos de sensores IoT

Todo mezclado, sin estructura definida.

---

## Data Lakehouse (Casa del Lago)

El Data Lakehouse es la arquitectura **m√°s moderna** (√∫ltimos 3-4 a√±os). 
Combina lo mejor de Data Warehouse y Data Lake.

### La Idea Central:
"¬øY si pudiera tener la flexibilidad y bajo costo del Data Lake, 
PERO con la calidad, velocidad y confiabilidad del Data Warehouse?"

### ¬øC√≥mo lo logra?
Mediante tecnolog√≠as como **Delta Lake**, **Apache Iceberg**, o **Apache Hudi** 
que agregan una "capa de gesti√≥n" sobre archivos simples.

### Caracter√≠sticas:
- **Storage econ√≥mico** (como Data Lake - archivos en cloud)
- **ACID Transactions** (como Data Warehouse - UPDATE, DELETE, INSERT confiables)
- **Schema Enforcement** (valida que los datos cumplan reglas)
- **Time Travel** (puedes ver versiones anteriores de los datos)
- **Optimizado para consultas** (√≠ndices, estad√≠sticas, particionamiento)

### Ventajas:
‚úÖ **Econ√≥mico** como Data Lake
‚úÖ **R√°pido** como Data Warehouse  
‚úÖ **Flexible** - soporta datos estructurados y semi-estructurados
‚úÖ **Confiable** - transacciones ACID, no hay corrupci√≥n de datos
‚úÖ **Versionado** - puedes volver atr√°s en el tiempo
‚úÖ **Unificado** - sirve para BI Y para Machine Learning

### Desventajas:
‚ùå **M√°s complejo de configurar** (requiere conocimientos t√©cnicos)
‚ùå **Tecnolog√≠a relativamente nueva** (menos madurez que Data Warehouse)
‚ùå **Requiere herramientas espec√≠ficas** (Databricks, Spark, etc.)

### Ejemplo Real con Delta Lake:
```python
# Puedes hacer UPDATE directamente en archivos (¬°imposible en Data Lake tradicional!)
deltaTable.update(
  condition = "fecha < '2024-01-01'", 
  set = {"estado": "'procesado'"}
)

# Puedes ver versiones anteriores (Time Travel)
df = spark.read.format("delta") \
  .option("versionAsOf", 5) \
  .load("/data/ventas")

# Puedes hacer MERGE (UPSERT) at√≥mico
deltaTable.merge(nuevos_datos, "id = id_nuevo") \
  .whenMatchedUpdate(...) \
  .whenNotMatchedInsert(...) \
  .execute()
```

### ¬øPor qu√© es el futuro?
Porque resuelve el problema hist√≥rico de tener que elegir entre:
- **Barato pero desordenado** (Data Lake)
- **Ordenado pero caro** (Data Warehouse)

Con Lakehouse tienes **barato Y ordenado**. üéØ

---

## Tabla Comparativa

| Caracter√≠stica | Data Warehouse | Data Lake | Data Lakehouse |
|----------------|----------------|-----------|----------------|
| **Tipos de datos** | Solo estructurados | Todos | Todos |
| **Formato** | Tablas en BD | Archivos (CSV, JSON, Parquet) | Archivos + capa de gesti√≥n (Delta) |
| **Esquema** | Schema-on-write (fijo) | Schema-on-read (flexible) | Schema enforcement (validado) |
| **Costo** | Alto | Bajo | Bajo-Medio |
| **Velocidad consultas** | Muy r√°pida | Lenta | R√°pida |
| **ACID** | S√≠ | No | S√≠ |
| **Calidad datos** | Alta | Baja (riesgo pantano) | Alta |
| **Uso principal** | BI, Reportes | Data Science, ML | BI + ML + Todo |
| **Usuarios** | Analistas negocio | Data Scientists, DE | Todos |
| **Ejemplos** | Snowflake, BigQuery | AWS S3, Azure Blob | Databricks, Delta Lake |

---

## Diagrama Visual

<img width="1200" height="630" alt="image" src="https://github.com/user-attachments/assets/5f8412d3-faef-4b44-ba83-575b62324eeb" />
Fuente: Databricks

    ‚Üë Lo mejor de ambos mundos
```
# SECCI√ìN 2: Medallion Architecture (Bronze ‚Üí Silver ‚Üí Gold)

## ¬øQu√© es Medallion Architecture?

Medallion Architecture describe una serie de **capas de datos** que denotan 
la **calidad y el nivel de procesamiento** de los datos almacenados en un 
Data Lakehouse.

Se usa para **organizar los datos l√≥gicamente** a medida que son procesados, 
desde su estado m√°s crudo hasta su forma m√°s refinada y lista para consumo 
empresarial.

## Objetivo Principal

**Mejorar de forma incremental y progresiva la calidad de los datos** a medida 
que fluyen a trav√©s de cada capa de la arquitectura.

Cada capa tiene un prop√≥sito espec√≠fico y usuarios espec√≠ficos.

---

## ü•â Capa BRONZE (Bronce) - Datos Crudos

### ¬øQu√© es?
La capa Bronze es la **zona de aterrizaje** donde llegan los datos tal cual 
vienen de las fuentes originales, sin transformaciones.

### Caracter√≠sticas:
- **Datos raw (crudos):** Sin limpiar, sin validar, sin transformar
- **De TODO tipo:** Estructurados, semi-estructurados, no estructurados
- **Hist√≥rico completo:** Se guardan TODOS los datos que llegan
- **Inmutable:** Una vez guardado, no se modifica (solo se agrega)
- **Con metadata:** Fecha de ingesta, fuente origen, versi√≥n

### ¬øPara qu√© sirve?
- **Backup hist√≥rico:** Siempre puedes volver a la fuente original
- **Reprocesamiento:** Si algo falla en capas superiores, reprocesas desde Bronze
- **Auditor√≠a:** Tienes registro de qu√© datos llegaron y cu√°ndo
- **Data Science exploratorio:** A veces necesitas los datos crudos

### Ejemplo Real - Retail:
```
üì¶ bronze/
  ‚îú‚îÄ‚îÄ ventas_pos/
  ‚îÇ   ‚îú‚îÄ‚îÄ 2024-11-29/
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tienda_001.json  ‚Üê Datos tal cual salen del POS
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tienda_002.json  ‚Üê Pueden tener errores, duplicados
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tienda_003.json  ‚Üê Diferentes formatos incluso
  ‚îÇ   ‚îî‚îÄ‚îÄ metadata/
  ‚îÇ       ‚îî‚îÄ‚îÄ ingesta_log.json  ‚Üê Registro de qu√© se ingiri√≥
  ‚îú‚îÄ‚îÄ clickstream_web/
  ‚îÇ   ‚îî‚îÄ‚îÄ 2024-11-29/
  ‚îÇ       ‚îî‚îÄ‚îÄ events.json  ‚Üê Logs raw del sitio web
  ‚îî‚îÄ‚îÄ imagenes_productos/
      ‚îî‚îÄ‚îÄ nuevos/  ‚Üê Im√°genes sin procesar
```

### ¬øQui√©n lo usa?
- Data Engineers (para debugging)
- Data Scientists (para an√°lisis exploratorio)
- Sistemas automatizados (para reprocesar)

### Formato recomendado:
- **Delta Lake** (para ACID y versionado)
- **Particionado por fecha** (ej: year=2024/month=11/day=29)
- **Compresi√≥n:** Snappy o Gzip

---

## ü•à Capa SILVER (Plata) - Datos Limpios

### ¬øQu√© es?
La capa Silver contiene datos **limpios, validados y enriquecidos**, pero 
a√∫n en un nivel t√©cnico (no modelado para negocio).

### Caracter√≠sticas:
- **Datos limpios:** Sin duplicados, sin nulos cr√≠ticos
- **Validados:** Pasan reglas de calidad
- **Normalizados:** Formatos estandarizados (fechas, monedas, etc.)
- **Enriquecidos:** Pueden tener joins con otras tablas
- **Tipados correctamente:** String, Int, Date, etc. bien definidos

### Transformaciones t√≠picas:
```python
# Ejemplo de transformaciones Bronze ‚Üí Silver

# 1. Limpiar datos
df_silver = df_bronze \
  .dropDuplicates(["transaction_id"]) \  # Eliminar duplicados
  .filter(col("monto") > 0) \             # Quitar montos negativos/cero
  .filter(col("fecha").isNotNull()) \     # Quitar registros sin fecha
  
# 2. Normalizar formatos
  .withColumn("fecha", to_date("fecha", "yyyy-MM-dd")) \  # Estandarizar fechas
  .withColumn("monto", round(col("monto"), 2)) \          # 2 decimales
  
# 3. Validar reglas de negocio
  .filter(col("cantidad") <= 1000) \  # Cantidad m√°xima razonable
  .filter(col("monto") <= 10000000) \ # Monto m√°ximo razonable
  
# 4. Enriquecer
  .join(productos_dim, "producto_id") \  # Agregar info de producto
  .join(tiendas_dim, "tienda_id")        # Agregar info de tienda
```

### Ejemplo Real - Retail:
```
üì¶ silver/
  ‚îú‚îÄ‚îÄ ventas_consolidadas/
  ‚îÇ   ‚îî‚îÄ‚îÄ ventas_limpias.delta  ‚Üê Todas las tiendas, limpio, sin duplicados
  ‚îú‚îÄ‚îÄ clientes_enriquecidos/
  ‚îÇ   ‚îî‚îÄ‚îÄ clientes.delta  ‚Üê Con segmentaci√≥n, RFM, etc.
  ‚îî‚îÄ‚îÄ productos_master/
      ‚îî‚îÄ‚îÄ productos.delta  ‚Üê Cat√°logo limpio y actualizado
```

### ¬øPara qu√© sirve?
- **Base para an√°lisis t√©cnico:** Data Scientists trabajan aqu√≠
- **Feature engineering para ML:** Se crean features desde Silver
- **Integraci√≥n de sistemas:** Otros sistemas pueden consumir Silver
- **Base para capa Gold:** Gold se construye desde Silver

### ¬øQui√©n lo usa?
- Data Engineers (construyen pipelines)
- Data Scientists (entrenan modelos)
- Sistemas automatizados (APIs, integraciones)

---

## ü•á Capa GOLD (Oro) - Datos Modelados para Negocio

### ¬øQu√© es?
La capa Gold contiene datos **altamente refinados, agregados y modelados** 
espec√≠ficamente para casos de uso de negocio.

### Caracter√≠sticas:
- **Modelado dimensional:** Esquemas estrella o copo de nieve
- **Agregaciones pre-calculadas:** KPIs, m√©tricas, totales
- **Optimizado para BI:** Estructurado para dashboards y reportes
- **Lenguaje de negocio:** Nombres de columnas que entiende el negocio
- **Menos volumen:** Solo lo necesario para an√°lisis

### Ejemplos de datasets Gold:
```
üì¶ gold/
  ‚îú‚îÄ‚îÄ kpis_ventas_diarias/
  ‚îÇ   ‚îî‚îÄ‚îÄ ventas_agregadas.delta
  ‚îÇ       Columnas: fecha, region, categoria, 
  ‚îÇ                 total_ventas, total_unidades, 
  ‚îÇ                 ticket_promedio, num_transacciones
  ‚îÇ
  ‚îú‚îÄ‚îÄ dashboard_ejecutivo/
  ‚îÇ   ‚îî‚îÄ‚îÄ metricas_mensuales.delta
  ‚îÇ       Columnas: mes, ventas_totales, margen_bruto,
  ‚îÇ                 crecimiento_vs_a√±o_anterior
  ‚îÇ
  ‚îú‚îÄ‚îÄ segmentacion_clientes/
  ‚îÇ   ‚îî‚îÄ‚îÄ clientes_rfm.delta
  ‚îÇ       Columnas: cliente_id, segmento_rfm, 
  ‚îÇ                 valor_lifetime, probabilidad_churn
  ‚îÇ
  ‚îî‚îÄ‚îÄ reporte_inventario/
      ‚îî‚îÄ‚îÄ stock_por_tienda.delta
          Columnas: tienda, producto, stock_actual,
                    dias_stock, punto_reorden
```

### Ejemplo de transformaci√≥n Silver ‚Üí Gold:
```python
# Crear tabla agregada para dashboard de ventas

df_gold_ventas_diarias = df_silver_ventas \
  .groupBy("fecha", "region", "categoria") \
  .agg(
    sum("monto").alias("total_ventas"),
    sum("cantidad").alias("total_unidades"),
    avg("monto").alias("ticket_promedio"),
    countDistinct("transaction_id").alias("num_transacciones"),
    countDistinct("cliente_id").alias("clientes_unicos")
  ) \
  .withColumn("a√±o", year("fecha")) \
  .withColumn("mes", month("fecha")) \
  .withColumn("trimestre", quarter("fecha"))

# Guardar en Gold
df_gold_ventas_diarias.write \
  .format("delta") \
  .mode("overwrite") \
  .partitionBy("a√±o", "mes") \
  .save("/gold/kpis_ventas_diarias")
```

### ¬øPara qu√© sirve?
- **Dashboards en Power BI / Tableau**
- **Reportes ejecutivos**
- **An√°lisis de negocio ad-hoc**
- **KPIs para monitoreo**

### ¬øQui√©n lo usa?
- **Analistas de negocio**
- **Ejecutivos (C-level)**
- **Product Managers**
- **Equipos de ventas/marketing**

**Caracter√≠stica clave:** Estos usuarios **NO saben SQL avanzado ni Python**. 
Necesitan datos ya procesados y listos para consumir.

---

## üìä Comparaci√≥n de las 3 Capas

| Aspecto | Bronze ü•â | Silver ü•à | Gold ü•á |
|---------|----------|----------|---------|
| **Estado datos** | Crudos, raw | Limpios, validados | Agregados, modelados |
| **Volumen** | Muy alto (TB-PB) | Alto (GB-TB) | Bajo-Medio (MB-GB) |
| **Calidad** | Baja (todo entra) | Media-Alta | Muy alta |
| **Transformaci√≥n** | Ninguna | Limpieza + validaci√≥n | Agregaci√≥n + modelado |
| **Usuarios** | DE, DS avanzados | DE, DS | Analistas, negocio |
| **Actualizaci√≥n** | Cada ingesta | Diaria/horaria | Diaria/semanal |
| **Prop√≥sito** | Backup + auditor√≠a | Base para an√°lisis | BI + dashboards |
| **Esquema** | Flexible | Estructurado | Dimensional |
| **Ejemplo tabla** | ventas_raw | ventas_limpias | kpi_ventas_diarias |

---

## üîÑ Flujo Completo: Bronze ‚Üí Silver ‚Üí Gold

### Ejemplo: Procesamiento de Ventas Retail
```
FUENTE: Sistemas POS de 500 tiendas
    ‚Üì
    ‚Üì (Ingesta cada 15 minutos)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BRONZE - Datos Raw                 ‚îÇ
‚îÇ  - 500 archivos JSON por d√≠a        ‚îÇ
‚îÇ  - Con errores, duplicados          ‚îÇ
‚îÇ  - Formatos inconsistentes          ‚îÇ
‚îÇ  - ~10 GB/d√≠a                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚Üì (Pipeline de limpieza - cada hora)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SILVER - Datos Limpios             ‚îÇ
‚îÇ  - 1 tabla consolidada              ‚îÇ
‚îÇ  - Sin duplicados                   ‚îÇ
‚îÇ  - Formatos estandarizados          ‚îÇ
‚îÇ  - Enriquecido con dimensiones      ‚îÇ
‚îÇ  - ~5 GB/d√≠a (comprimido)           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚Üì (Agregaciones - cada 6 horas)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GOLD - KPIs y M√©tricas             ‚îÇ
‚îÇ  - Ventas por regi√≥n/d√≠a            ‚îÇ
‚îÇ  - Top productos                    ‚îÇ
‚îÇ  - Segmentaci√≥n clientes            ‚îÇ
‚îÇ  - Dashboard ejecutivo              ‚îÇ
‚îÇ  - ~100 MB/d√≠a                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚Üì
    ‚Üì
Power BI Dashboard ‚Üê Analistas de negocio
```

---

## üí° Mejores Pr√°cticas

### 1. **Inmutabilidad en Bronze**
‚ùå Nunca modifiques datos en Bronze
‚úÖ Siempre agrega nuevos datos con timestamp

### 2. **Idempotencia en pipelines**
Los pipelines Silver y Gold deben ser **idempotentes**: 
si los ejecutas 2 veces con los mismos datos de entrada, 
el resultado debe ser el mismo.

### 3. **Versionado con Delta Lake**
Usa Delta Lake en todas las capas para:
- Time Travel (volver a versiones anteriores)
- ACID transactions (no corrupci√≥n de datos)
- Schema evolution (agregar columnas sin romper nada)

### 4. **Particionamiento inteligente**
```
Bronze:  Particionar por fecha de ingesta
         /bronze/ventas/year=2024/month=11/day=29/

Silver:  Particionar por fecha l√≥gica de negocio
         /silver/ventas/fecha=2024-11-29/

Gold:    Particionar por dimensiones de consulta frecuente
         /gold/ventas/region=santiago/year=2024/month=11/
```

### 5. **Data Quality Checks**
Implementar validaciones autom√°ticas:
```python
# Ejemplo con Great Expectations
expectation_suite = df.expect_column_values_to_not_be_null("transaction_id")
expectation_suite = df.expect_column_values_to_be_between("monto", 0, 10000000)
expectation_suite = df.expect_column_values_to_be_in_set("estado", ["completado", "cancelado"])
```

---

## üéØ Caso Real: Mi experiencia con Medallion Architecture

[Aqu√≠ puedes agregar un p√°rrafo sobre tu experiencia en Cencosud]

Ejemplo:
```
En Cencosud, implementamos una arquitectura similar (aunque no 
la llam√°bamos "Medallion" en ese momento). 

Ten√≠amos:
- **Zona Raw:** Donde llegaban los datos de los POS de todas las tiendas
- **Zona Procesada:** Datos limpios y consolidados
- **Zona Analytics:** Tablas agregadas para dashboards

El mayor desaf√≠o era mantener la calidad en la zona Raw - a veces 
entraban datos corruptos que romp√≠an los pipelines downstream.

Si lo dise√±ara ahora, usar√≠a:
1. Delta Lake en todas las capas (para versionado y rollback)
2. Data quality checks autom√°ticos antes de pasar Bronze ‚Üí Silver
3. Alertas cuando la calidad de datos baja de cierto threshold
4. Unity Catalog para gobernanza y lineage
```

---

## üìê Diagrama Medallion Architecture

[Incluye el diagrama que compartiste de Databricks]

**Fuente:** Databricks - Data Lakehouse Architecture

El diagrama muestra claramente c√≥mo:
- Bronze acepta TODO tipo de datos
- Silver limpia y valida
- Gold agrega y modela para consumo empresarial
