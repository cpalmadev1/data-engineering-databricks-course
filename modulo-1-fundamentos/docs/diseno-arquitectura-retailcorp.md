# Dise√±o de Arquitectura Data Engineering
## RetailCorp Chile - Propuesta T√©cnica

**Autor:** C√©sar Palma  
**Fecha:** 2 de Diciembre, 2024  
**Versi√≥n:** 1.0

---

## 1. Resumen Ejecutivo

RetailCorp Chile requiere modernizar su infraestructura de datos para 
soportar an√°lisis en tiempo real, reporting ejecutivo, y casos de uso 
avanzados de Machine Learning.

La soluci√≥n propuesta implementa una **arquitectura Data Lakehouse** 
utilizando **Medallion Architecture** (Bronze/Silver/Gold) sobre 
**Delta Lake**, con dise√±o **cloud-agnostic** que funciona en 
Azure, AWS, o GCP.

Esta arquitectura permitir√°:
- Dashboard ejecutivo con latencia < 1 minuto
- Reducci√≥n de costos de storage vs soluci√≥n actual
- Escalabilidad a petabytes sin redise√±o
- Soporte para BI y ML desde misma plataforma

**Costo estimado:** $8,000-9,500 USD/mes (depende del cloud provider)

---

## 2. Contexto del Negocio

### 2.1 Situaci√≥n Actual

RetailCorp opera:
- 500 tiendas f√≠sicas (Arica a Punta Arenas)
- 1 e-commerce (retailcorp.cl)
- 1 app m√≥vil (iOS + Android)
- 5M miembros programa fidelizaci√≥n

**Volumen de datos:**
- 50M transacciones/mes (~1.6M/d√≠a)
- 2M visitas web diarias
- 500K usuarios activos app

**Problemas actuales:**
- Reportes tardan 2-3 d√≠as
- Sin visibilidad tiempo real de inventario
- Datos en silos (POS, web, app separados)
- Imposible hacer an√°lisis predictivo

### 2.2 Objetivos del Proyecto

1. **Dashboard ejecutivo** actualizado cada minuto
2. **An√°lisis diario inventario** listo 7 AM
3. **Reportes mensuales** autom√°ticos
4. **Sistema recomendaciones** ML
5. **Detecci√≥n anomal√≠as** en tiempo real

---

## 3. An√°lisis de Requerimientos

[AQU√ç COPIAS EL AN√ÅLISIS QUE YA HICIMOS]

| Req | Descripci√≥n | Latencia | Procesamiento | Justificaci√≥n |
|-----|-------------|----------|---------------|---------------|
| 1 | Dashboard Ejecutivo RT | < 1 min | Streaming | CEO necesita ver ventas actuales |
| 2 | Inventario Diario | 7 AM | Batch | Datos pueden esperar noche |
| 3 | Reportes Mensuales | D√≠a 1 mes | Batch | No cr√≠tico tiempo real |
| 4 | Recomendaciones ML | < 1 seg lectura | Batch entrenamiento | Modelo pre-calculado |
| 5 | Detecci√≥n Fraude | < 2 seg | Streaming | Cr√≠tico actuar inmediato |

---

## 4. Decisiones Arquitect√≥nicas

### 4.1 Storage: Data Lakehouse

**Decisi√≥n:** Implementar **Data Lakehouse** con **Delta Lake**

**Alternativas consideradas:**
- ‚ùå Data Warehouse: Inflexible, caro, solo datos estructurados
- ‚ùå Data Lake tradicional: Riesgo de "data swamp", sin ACID
- ‚úÖ Data Lakehouse: Combina flexibilidad + calidad

**Justificaci√≥n:**

RetailCorp necesita:
- ‚úÖ Almacenar datos estructurados (ventas) Y no estructurados (im√°genes)
- ‚úÖ Soportar BI (dashboards) Y ML (recomendaciones)
- ‚úÖ ACID transactions (para UPDATE inventario)
- ‚úÖ Costos bajos de storage

**Solo Data Lakehouse cumple todos estos requisitos.**

**Tecnolog√≠a:**
- Formato: **Delta Lake** (open source)
- Storage: ADLS Gen2 (Azure) / S3 (AWS) / Cloud Storage (GCP)
- Costo: ~$0.02/GB/mes

---

### 4.2 Medallion Architecture

**Decisi√≥n:** Implementar arquitectura de 3 capas (Bronze/Silver/Gold)

**Estructura:**
```
ü•â BRONZE (Raw Zone) - ~200MB/d√≠a
‚îú‚îÄ Datos crudos sin transformar
‚îú‚îÄ Formato original (CSV, JSON)
‚îú‚îÄ Inmutable (append-only)
‚îú‚îÄ Particionado: /year=YYYY/month=MM/day=DD/
‚îî‚îÄ Retenci√≥n: 365 d√≠as

ü•à SILVER (Cleaned Zone) - ~150MB/d√≠a
‚îú‚îÄ Datos limpios y validados
‚îú‚îÄ Sin duplicados
‚îú‚îÄ Formato Delta Lake
‚îú‚îÄ Schema enforcement
‚îî‚îÄ Retenci√≥n: 365 d√≠as

ü•á GOLD (Business Zone) - ~20MB/d√≠a
‚îú‚îÄ Agregaciones pre-calculadas
‚îú‚îÄ Modelado dimensional
‚îú‚îÄ Optimizado para BI/ML
‚îî‚îÄ Retenci√≥n: 1,095 d√≠as (3 a√±os)
```

**Justificaci√≥n:**

- ‚úÖ **Separation of concerns:** Cada capa tiene prop√≥sito claro
- ‚úÖ **Reprocessing:** Si algo falla en Silver/Gold, reprocesas desde Bronze
- ‚úÖ **Auditor√≠a:** Bronze guarda datos originales
- ‚úÖ **Performance:** Gold optimizado para consultas r√°pidas
- ‚úÖ **Costos:** Compresi√≥n reduce de 200MB ‚Üí 20MB

---

### 4.3 Batch vs Streaming

**Decisi√≥n:** Arquitectura **h√≠brida** (Lambda Architecture)

| Caso de Uso | Procesamiento | Frecuencia | Justificaci√≥n |
|-------------|---------------|------------|---------------|
| Dashboard RT | **Streaming** | Continuo | CEO requiere datos < 1 min |
| Inventario | **Batch** | 01:00 AM | Puede esperar horas |
| Reportes | **Batch** | Mensual | No cr√≠tico tiempo real |
| ML Training | **Batch** | Semanal | Modelos se entrenan offline |
| ML Serving | **Lectura** | < 100ms | Leer tabla pre-calculada |
| Fraude | **Streaming** | Continuo | Cr√≠tico bloquear inmediato |
| Anomal√≠as | **Batch** | Cada hora | No requiere acci√≥n inmediata |

**Trade-off de costos:**

| Opci√≥n | Costo/mes | Latencia | Cu√°ndo usar |
|--------|-----------|----------|-------------|
| Todo Streaming | $12,000 | Segundos | ‚ùå Sobre-ingenier√≠a |
| Todo Batch | $3,000 | Horas | ‚ùå No cumple req RT |
| **H√≠brido** | **$8,500** | **√ìptimo** | **‚úÖ Balance costo/beneficio** |

---

### 4.4 Stack Tecnol√≥gico Multi-Cloud

**Decisi√≥n:** Dise√±o **cloud-agnostic** con preferencia Azure (experiencia actual)

**Componentes principales:**

| Funci√≥n | Azure | AWS | GCP | Decisi√≥n |
|---------|-------|-----|-----|----------|
| Storage | ADLS Gen2 | S3 ‚≠ê | Cloud Storage | Cualquiera funciona |
| Compute | Databricks | Databricks | Dataproc | **Databricks** ‚úÖ |
| Streaming | Event Hubs | Kinesis | Pub/Sub | Depende del cloud |
| Orchestration | Databricks Workflows | Databricks Workflows | Composer | **Databricks** ‚úÖ |
| BI | Power BI ‚≠ê | QuickSight | Looker | **Power BI** (Latam) |

‚≠ê = M√°s com√∫n en el mercado

**Justificaci√≥n:**

1. **Databricks everywhere:** Mismo c√≥digo PySpark en Azure/AWS/GCP
2. **Delta Lake:** Open source, no vendor lock-in
3. **Power BI:** M√°s familiar para ejecutivos chilenos
4. **Portabilidad:** Cambiar de cloud = cambiar nombres herramientas, no arquitectura

**Recomendaci√≥n inicial: Azure**
- Equipo ya tiene experiencia
- Menor curva aprendizaje
- Integraci√≥n nativa con Power BI

**Plan futuro:** Si el negocio crece, evaluar AWS (m√°s econ√≥mico a gran escala)

---

## 5. Pipeline Ejemplo: Ventas

### 5.1 Flujo End-to-End

[AQU√ç DESCRIBES EL FLUJO QUE ANALIZAMOS]

**Fuentes:**
- POS tiendas: 500 tiendas √ó CSV cada 15 min
- E-commerce: PostgreSQL CDC cada 5 min
- App m√≥vil: JSON eventos real-time

**Bronze Layer:**
```
/bronze/
  ‚îú‚îÄ‚îÄ ventas_pos/year=2024/month=12/day=02/
  ‚îú‚îÄ‚îÄ ecommerce_transacciones/
  ‚îî‚îÄ‚îÄ app_eventos/
```

**Transformaciones Bronze ‚Üí Silver:**
1. Unificar esquema (3 fuentes ‚Üí 1 tabla)
2. Deduplicar (por transaction_id)
3. Validar (monto > 0, fecha v√°lida)
4. Normalizar (fechas ISO, moneda CLP)
5. Enriquecer (join productos, tiendas, clientes)

**Silver Layer:**
```
/silver/ventas_consolidadas/
Esquema:
- transaction_id (unique)
- fecha_hora (timestamp)
- canal (enum: POS/WEB/APP)
- producto_id
- cantidad
- monto
```

**Transformaciones Silver ‚Üí Gold:**

**Gold 1:** Dashboard Tiempo Real (Streaming)
```sql
SELECT 
  fecha_hora,
  canal,
  SUM(monto) AS ventas_totales,
  COUNT(*) AS num_transacciones,
  AVG(monto) AS ticket_promedio
FROM silver.ventas_consolidadas
WHERE fecha_hora >= current_timestamp - INTERVAL 1 MINUTE
GROUP BY fecha_hora, canal
```

**Gold 2:** Reportes Diarios (Batch)
```sql
SELECT 
  DATE(fecha_hora) AS fecha,
  producto_id,
  SUM(cantidad) AS unidades_vendidas,
  SUM(monto) AS ventas_totales,
  RANK() OVER (ORDER BY SUM(monto) DESC) AS ranking
FROM silver.ventas_consolidadas
WHERE DATE(fecha_hora) = CURRENT_DATE - 1
GROUP BY fecha, producto_id
```

### 5.2 Volumetr√≠a y Performance

**Vol√∫menes:**
- Input (Bronze): 1.6M registros/d√≠a = ~200MB/d√≠a
- Output (Silver): 1.6M registros/d√≠a = ~150MB/d√≠a (compresi√≥n Delta)
- Output (Gold): Agregados = ~20MB/d√≠a

**Tiempos de procesamiento:**
- Bronze ‚Üí Silver (1 hora datos): 2-3 minutos
- Silver ‚Üí Gold Streaming: < 1 minuto latencia
- Silver ‚Üí Gold Batch: 5-8 minutos (d√≠a completo)

---

## 6. Estimaci√≥n de Costos

### 6.1 Comparaci√≥n Multi-Cloud

| Componente | Azure | AWS | GCP |
|------------|-------|-----|-----|
| Storage (50TB) | $900/mes | $1,150/mes | $1,000/mes |
| Compute Batch | $1,200/mes | $1,000/mes | $1,100/mes |
| Compute Streaming | $4,000/mes | $3,500/mes | $3,800/mes |
| Event Platform | $800/mes | $600/mes | $700/mes |
| Databricks | $1,500/mes | $1,500/mes | $1,500/mes |
| BI Tools | $500/mes | $300/mes | $400/mes |
| Networking | $500/mes | $400/mes | $450/mes |
| **TOTAL** | **$9,400/mes** | **$8,450/mes** | **$8,950/mes** |

**AWS es ~10% m√°s econ√≥mico** a esta escala.

### 6.2 Optimizaciones de Costo

1. **Auto-scaling:** Clusters se apagan cuando no se usan
2. **Spot instances:** Reducci√≥n 60-70% en compute no cr√≠tico
3. **Cold storage:** Datos >90 d√≠as a tier econ√≥mico
4. **Particionamiento:** Queries solo leen datos necesarios

**Costo optimizado estimado: $7,000-7,500/mes**

---

## 7. Riesgos y Mitigaciones

| Riesgo | Probabilidad | Impacto | Mitigaci√≥n |
|--------|--------------|---------|------------|
| Data quality issues | Alta | Alto | Great Expectations + alertas |
| Costos exceden presupuesto | Media | Alto | Monitoring + alertas costos |
| Performance degradation | Media | Medio | Optimization (Z-Order, clustering) |
| Vendor lock-in | Baja | Alto | Delta Lake open source |
| Skills gap equipo | Media | Medio | Training + documentaci√≥n |

---

## 8. Diagramas

### 8.1 Arquitectura General End-to-End

![Arquitectura Data Lakehouse RetailCorp](../diagramas/arquitectura-general.png)

*Figura 1: Arquitectura completa cloud-agnostic mostrando flujo desde fuentes hasta consumo*

---

## 9. Conclusiones

La arquitectura propuesta cumple todos los requerimientos de RetailCorp:

‚úÖ Dashboard tiempo real (< 1 min latencia)  
‚úÖ Inventario diario automatizado (7 AM)  
‚úÖ Reportes mensuales ejecutivos  
‚úÖ Base para ML (recomendaciones, anomal√≠as)  
‚úÖ Escalable a crecimiento futuro  
‚úÖ Costo-efectiva ($8,000-9,000/mes)  
‚úÖ Cloud-agnostic (no vendor lock-in)

**Recomendaci√≥n de implementaci√≥n:**

**Fase 1 (Mes 1-2):** MVP
- Bronze + Silver para ventas
- Dashboard b√°sico streaming
- Azure (experiencia actual)

**Fase 2 (Mes 3-4):** Expansi√≥n
- Gold layer completo
- Todos los casos de uso
- Optimizaciones performance

**Fase 3 (Mes 5-6):** Advanced
- ML en producci√≥n
- Governance (Unity Catalog)
- Monitoring avanzado

---

## 10. Referencias

- [Databricks: What is a Data Lakehouse?](https://www.databricks.com/glossary/data-lakehouse)
- [Delta Lake Documentation](https://docs.delta.io/)
- [Azure Databricks Best Practices](https://learn.microsoft.com/azure/databricks/)
- [AWS EMR Best Practices](https://docs.aws.amazon.com/emr/)
- [Medallion Architecture](https://www.databricks.com/glossary/medallion-architecture)

---

**Fin del documento**
